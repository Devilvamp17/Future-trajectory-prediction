{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aaa64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1833095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, H]\n",
    "        weights = torch.softmax(self.attn(x), dim=1)  # [B, T, 1]\n",
    "        context = torch.sum(weights * x, dim=1)       # [B, H]\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class KalmanLSTM(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=2, output_len=5, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.output_len = output_len\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.attn = TemporalAttention(hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_len * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, 6]\n",
    "        lstm_out, _ = self.lstm(x)  # [B, T, H]\n",
    "        context, _ = self.attn(lstm_out)  # [B, H]\n",
    "        context = self.norm(context)\n",
    "        pred = self.fc(context)  # [B, output_len * 2]\n",
    "        return pred.view(-1, self.output_len, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fde15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset: X=(609284, 10, 6), Y=(609284, 5, 2)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load trajectory data ---\n",
    "with open(\"/workspace/hjs/python/lstm_train/car_trajectories.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "input_len, pred_len = 10, 5\n",
    "X_seqs, Y_seqs = [], []\n",
    "\n",
    "for track_id, points in data.items():\n",
    "    coords = np.array([[p[1], p[2]] for p in points])\n",
    "    if len(coords) < input_len + pred_len:\n",
    "        continue\n",
    "    for i in range(len(coords) - input_len - pred_len + 1):\n",
    "        X_seqs.append(coords[i:i+input_len])\n",
    "        Y_seqs.append(coords[i+input_len:i+input_len+pred_len])\n",
    "\n",
    "X = np.array(X_seqs)\n",
    "Y = np.array(Y_seqs)\n",
    "def add_kinematics(X):\n",
    "    \"\"\"\n",
    "    X: [B, T, 2] â†’ return [B, T, 6] with velocity and acceleration\n",
    "    \"\"\"\n",
    "    velocity = np.diff(X, axis=1, prepend=X[:, :1])  # [B, T, 2]\n",
    "    acceleration = np.diff(velocity, axis=1, prepend=velocity[:, :1])  # [B, T, 2]\n",
    "    return np.concatenate([X, velocity, acceleration], axis=-1)  # [B, T, 6]\n",
    "\n",
    "\n",
    "X = add_kinematics(X)  # Now shape: [B, T, 6]\n",
    "print(f\"Prepared dataset: X={X.shape}, Y={Y.shape}\")\n",
    "# --- Train/Val split ---\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DataLoaders ---\n",
    "BATCH_SIZE = 64\n",
    "train_ds = TensorDataset(torch.Tensor(X_train), torch.Tensor(Y_train))\n",
    "val_ds = TensorDataset(torch.Tensor(X_val), torch.Tensor(Y_val))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ab101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred, target, dt=1.0):\n",
    "    ade = torch.mean(torch.norm(pred - target, dim=-1))  # [B, N]\n",
    "    fde = torch.norm(pred[:, -1] - target[:, -1], dim=-1).mean()\n",
    "    mse = F.mse_loss(pred, target)\n",
    "    mae = F.l1_loss(pred, target)\n",
    "    physics_loss = compute_physics_loss(pred, dt)\n",
    "    return ade.item(), fde.item(), mse.item(), mae.item(), physics_loss.item()\n",
    "def compute_physics_loss(pred, dt=1.0):\n",
    "    \"\"\"\n",
    "    pred: [B, N, 2]\n",
    "    \"\"\"\n",
    "    velocity = (pred[:, 1:] - pred[:, :-1]) / dt  # [B, N-1, 2]\n",
    "    vel_diff = velocity[:, 1:] - velocity[:, :-1]  # acceleration diff\n",
    "    acc_loss = torch.mean(vel_diff**2)\n",
    "    return acc_loss\n",
    "def train_model(model, train_loader, val_loader=None, epochs=50, lr=1e-3, lambda_phy=0.1,\n",
    "                device='cuda', weight_decay=1e-5, early_stop_patience=5):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                           factor=0.5, patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_mse, total_phy = 0.0, 0.0, 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for x_batch, y_batch in pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_batch)\n",
    "\n",
    "            mse_loss = criterion(pred, y_batch)\n",
    "            physics_loss = compute_physics_loss(pred)\n",
    "            loss = mse_loss + lambda_phy * physics_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mse += mse_loss.item()\n",
    "            total_phy += physics_loss.item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"MSE\": f\"{mse_loss.item():.4f}\",\n",
    "                \"Phys\": f\"{physics_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_train_mse = total_mse / len(train_loader)\n",
    "        avg_train_phy = total_phy / len(train_loader)\n",
    "\n",
    "        # --- Validation phase ---\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss, val_mse_total, val_mae_total = 0.0, 0.0, 0.0\n",
    "            val_phy_total, val_ade_total, val_fde_total = 0.0, 0.0, 0.0\n",
    "            count = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                    pred = model(x_val)\n",
    "\n",
    "                    mse = criterion(pred, y_val)\n",
    "                    phy = compute_physics_loss(pred)\n",
    "                    loss = mse + lambda_phy * phy\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    ade, fde, mse_val, mae_val, phy_val = compute_metrics(pred, y_val)\n",
    "                    val_ade_total += ade * len(x_val)\n",
    "                    val_fde_total += fde * len(x_val)\n",
    "                    val_mse_total += mse_val * len(x_val)\n",
    "                    val_mae_total += mae_val * len(x_val)\n",
    "                    val_phy_total += phy_val * len(x_val)\n",
    "                    count += len(x_val)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            avg_val_ade = val_ade_total / count\n",
    "            avg_val_fde = val_fde_total / count\n",
    "            avg_val_mse = val_mse_total / count\n",
    "            avg_val_mae = val_mae_total / count\n",
    "            avg_val_phy = val_phy_total / count\n",
    "\n",
    "            print(f\"âœ… Epoch {epoch+1:02d}/{epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} (MSE={avg_train_mse:.4f}, Phys={avg_train_phy:.4f}) | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f} | MSE: {avg_val_mse:.4f} | MAE: {avg_val_mae:.4f} | \"\n",
    "                  f\"ADE: {avg_val_ade:.4f} | FDE: {avg_val_fde:.4f} | Phys: {avg_val_phy:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                best_model = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stop_patience:\n",
    "                    print(\"ðŸ›‘ Early stopping triggered.\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"âœ… Epoch {epoch+1:02d}/{epochs} | Train Loss: {avg_train_loss:.4f} \"\n",
    "                  f\"(MSE={avg_train_mse:.4f}, Phys={avg_train_phy:.6f})\")\n",
    "\n",
    "    if best_model:\n",
    "        torch.save(best_model, \"best_kalman_lstm.pt\")\n",
    "        print(\"ðŸ“¦ Best model saved to 'best_kalman_lstm.pt'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = KalmanLSTM(input_size=6, hidden_size=128, num_layers=2, output_len=5, dropout=0.3)\n",
    "# train_model(model, train_dl, val_dl,\n",
    "#             epochs=100, lr=1e-3, lambda_phy=0.1,\n",
    "#             device=device, weight_decay=1e-5, early_stop_patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfe9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 08:04:56,655] A new study created in memory with name: no-name-94a5eb2a-8165-4963-a56f-96d38654db27\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 01/25 | Train Loss: 0.0634 (MSE=0.0603, Phys=0.0033) | Val Loss: 0.0212 | MSE: 0.0205 | MAE: 0.0882 | ADE: 0.1370 | FDE: 0.1416 | Phys: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 02/25 | Train Loss: 0.0212 (MSE=0.0201, Phys=0.0012) | Val Loss: 0.0223 | MSE: 0.0190 | MAE: 0.0788 | ADE: 0.1325 | FDE: 0.1396 | Phys: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 03/25 | Train Loss: 0.0193 (MSE=0.0185, Phys=0.0009) | Val Loss: 0.0294 | MSE: 0.0288 | MAE: 0.1098 | ADE: 0.1912 | FDE: 0.2188 | Phys: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 04/25 | Train Loss: 0.0184 (MSE=0.0178, Phys=0.0006) | Val Loss: 0.1929 | MSE: 0.1926 | MAE: 0.3013 | ADE: 0.5575 | FDE: 0.6599 | Phys: 0.0004\n",
      "ðŸ›‘ Early stopping triggered.\n",
      "ðŸ“¦ Best model saved to 'best_kalman_lstm.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-24 08:06:24,310] Trial 0 finished with value: 0.19255837166423911 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3170172683007433, 'lr': 0.004286336079681596, 'weight_decay': 3.5173290040063774e-05, 'lambda_phy': 0.9356935627164077}. Best is trial 0 with value: 0.19255837166423911.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3948819060802883 and num_layers=1\n",
      "  warnings.warn(\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 01/25 | Train Loss: 0.3336 (MSE=0.3328, Phys=0.0009) | Val Loss: 0.0104 | MSE: 0.0101 | MAE: 0.0390 | ADE: 0.0654 | FDE: 0.0845 | Phys: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 02/25 | Train Loss: 0.0096 (MSE=0.0095, Phys=0.0002) | Val Loss: 0.0088 | MSE: 0.0088 | MAE: 0.0336 | ADE: 0.0585 | FDE: 0.0857 | Phys: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 03/25 | Train Loss: 0.0087 (MSE=0.0086, Phys=0.0001) | Val Loss: 0.0079 | MSE: 0.0076 | MAE: 0.0284 | ADE: 0.0487 | FDE: 0.0729 | Phys: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 04/25 | Train Loss: 0.0083 (MSE=0.0082, Phys=0.0001) | Val Loss: 0.0077 | MSE: 0.0077 | MAE: 0.0305 | ADE: 0.0532 | FDE: 0.0708 | Phys: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 05/25 | Train Loss: 0.0080 (MSE=0.0079, Phys=0.0001) | Val Loss: 0.0080 | MSE: 0.0078 | MAE: 0.0346 | ADE: 0.0569 | FDE: 0.0824 | Phys: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 06/25 | Train Loss: 0.0078 (MSE=0.0076, Phys=0.0001) | Val Loss: 0.0128 | MSE: 0.0127 | MAE: 0.0581 | ADE: 0.1068 | FDE: 0.1365 | Phys: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 07/25 | Train Loss: 0.0076 (MSE=0.0075, Phys=0.0001) | Val Loss: 0.0077 | MSE: 0.0076 | MAE: 0.0326 | ADE: 0.0559 | FDE: 0.0776 | Phys: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 08/25 | Train Loss: 0.0074 (MSE=0.0073, Phys=0.0001) | Val Loss: 0.0072 | MSE: 0.0071 | MAE: 0.0284 | ADE: 0.0495 | FDE: 0.0749 | Phys: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 09/25 | Train Loss: 0.0073 (MSE=0.0072, Phys=0.0001) | Val Loss: 0.0072 | MSE: 0.0071 | MAE: 0.0279 | ADE: 0.0489 | FDE: 0.0739 | Phys: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 10/25 | Train Loss: 0.0072 (MSE=0.0071, Phys=0.0001) | Val Loss: 0.0076 | MSE: 0.0073 | MAE: 0.0297 | ADE: 0.0523 | FDE: 0.0758 | Phys: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 11/25 | Train Loss: 0.0071 (MSE=0.0070, Phys=0.0001) | Val Loss: 0.0084 | MSE: 0.0084 | MAE: 0.0347 | ADE: 0.0619 | FDE: 0.0868 | Phys: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7139/7617 [00:19<00:01, 361.13it/s, Loss=0.0027, MSE=0.0026, Phys=0.0001]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [64, 128, 256])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    lambda_phy = trial.suggest_float(\"lambda_phy\", 0.01, 1.0)\n",
    "\n",
    "    model = KalmanLSTM(input_size=6, hidden_size=hidden_size,\n",
    "                       num_layers=num_layers, output_len=5, dropout=dropout)\n",
    "    \n",
    "    # Train for fewer epochs to keep tuning fast\n",
    "    train_model(model, train_dl, val_dl,\n",
    "                epochs=25, lr=lr, lambda_phy=lambda_phy,\n",
    "                device=device, weight_decay=weight_decay, early_stop_patience=7)\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(\"best_kalman_lstm.pt\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_mse = 0.0\n",
    "    count = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_dl:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            pred = model(x_val)\n",
    "            val_mse += criterion(pred, y_val).item() * len(x_val)\n",
    "            count += len(x_val)\n",
    "\n",
    "    return val_mse / count  # Minimize this\n",
    "\n",
    "# Start optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"ðŸ† Best hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(f\"Best Validation MSE: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize_study(study):\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\n",
    "            \"trial\": t.number,\n",
    "            \"state\": t.state.name,\n",
    "            \"value (Val MSE)\": t.value,\n",
    "            \"hidden_size\": t.params.get(\"hidden_size\"),\n",
    "            \"num_layers\": t.params.get(\"num_layers\"),\n",
    "            \"dropout\": round(t.params.get(\"dropout\", 0), 3),\n",
    "            \"lr\": f\"{t.params.get('lr'):.1e}\",\n",
    "            \"weight_decay\": f\"{t.params.get('weight_decay'):.1e}\",\n",
    "            \"lambda_phy\": round(t.params.get(\"lambda_phy\", 0), 3)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(\"value (Val MSE)\").reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nðŸ“‹ Trial Summary (sorted by best Val MSE):\")\n",
    "    print(df.head(10).to_string(index=False))  # Show top 10 results\n",
    "\n",
    "    # Save to CSV\n",
    "#     df.to_csv(\"optuna_trial_summary.csv\", index=False)\n",
    "#     print(\"\\nðŸ“ Full trial summary saved to 'optuna_trial_summary.csv'.\")\n",
    "\n",
    "summarize_study(study)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
